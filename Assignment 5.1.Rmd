---
title: "Assignment 5"
author: "SAMIKSHA VENUGOPAL"
date: "11/29/2021"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


```{r}

```
```{r}
## loading the packages 
library(cluster)
library(caret)
library(tidyverse)
library(factoextra)
library(ISLR)
library(dendextend)
library(knitr)
library(factoextra)
library(readr)
set.seed(111)

```

```{r}
## Importing Cereals dataset
setwd("/users/Samikshachalla/downloads")
Cereals<-read.csv("Cereals.csv")
head(Cereals)
```

```{r}


##Q) Data Prepocessing .Removing all cereals with missing valuees.

# Removing missing values that might be present in cereals dataset

sum(is.na(Cereals))
Cereals<-na.omit(Cereals)
sum(is.na(Cereals))
Cereals1<-Cereals[,c(-1,-2,-3)]


```
```{r}
#Scaling the data
colMeans(is.na(Cereals))
head(Cereals1)
summary(Cereals1)
Cereals_scaled<-(scale(Cereals1))

```



```{r}
## Scaling the data


#1)Apply hierachical clusting to the data using Euclidean distance

## Applying hierachical clustering to the data using Euclidean distance
# Dissimilarity matrix


Distance <- dist(Cereals1,method = "euclidean")


# Hierachical clustering using Complete Linkage

H_complete<-hclust(Distance,method = "complete")

# Plot the obtained dendogram

plot(H_complete,cex =0.6,hang =-1)

##Use Agnes to compare the clusting from single linkage, average linkage and ward

cluster_single<-agnes(Cereals1,method = "single")

cluster_complete<-agnes(Cereals1,method = "complete")

cluster_average <-agnes(Cereals1,method = "average")

 cluster_ward<- agnes(Cereals1,method = "ward")

 
 # Comparing the agglomerative co-efficient of single , complete, average and ward methods
cluster_single$ac
cluster_complete$ac
cluster_average$ac
 cluster_ward$ac
 
 # By observing the above values we can say that the best linkage method is ward with aggalomerative co-efficient of 0.9502095
 
 
 ## visulizing the dendogram using wards methods:
 
 
pl<-pltree(cluster_ward,cex=0.6,hang=-1,main = "dendrogram of agnes-wards method")
 
```
 
 
 

 
 


```{r}
#2) How many clusters would you choose?

## Create the distance matrix

dc<-dist(Cereals_scaled,method = "euclidean")


 
 

```

```{r}
# Wards method for Hierarchical clusting

w_hc<-hclust(dc,method = "ward.D2")

# Plotting dendrogram and taking k=2 by oberving the distance
plot(w_hc,cex=0.6)
rect.hclust(w_hc,k=2,border = 1:2)

# For identifying clusters , cut the dendrogram with cutree()
clust<-cutree(w_hc,k=2)

#Number of member in each cluster

table(clust)
#k=2 is cutting the longest path , so I choose K=2.
```



```{r}
# 3) Comment on the same structure of the clusters and on their ##stability.

#To compare two dendrogram -we use

library(dendextend)


Cereals_NewOne<-Cereals

## Removing any missing values that might be present in the data
 
 
 Nd<-na.omit(Cereals_NewOne)
 Nd1<-Nd[,c(-1,-2,-3)]
 Nd2<-scale(Nd1)
 Nd3<-as.data.frame(Nd2)
 
 # Divide the data and create parition 
 p1<-Nd[1:55,]
 p2<-Nd[56:74,]
 
 #Perform clustering using () with single,complete
 
 r1<-agnes(scale(p1[,-c(1:3)]),method = "ward")
 r2<-agnes(scale(p1[,-c(1:3)]),method = "average")
 r3<-agnes(scale(p1[,-c(1:3)]),method = "complete")
 r4<-agnes(scale(p1[,-c(1:3)]),method = "single")
 cbind(ward=r1$ac,average=r2$ac,complete=r3$ac,single=r4$ac)
 
 c2<-cutree(r1,k=2)
 
 
 
 
 
 ## Calculate the centres
 
 cc<-as.data.frame(cbind(scale(p1[,-c(1:3)]),c2))
 centre1<-colMeans(cc[cc$c2==1,])
 centre2<-colMeans(cc[cc$c2==2,])
 
 #Bind the 2 centres
 
 centre<-rbind(centre1,centre2)
 centre
 
```
 
 

```{r}
 ## Calculating Distance 


x<-as.data.frame(rbind(centre[,-14],scale(p2[,-c(1:3)])))
 y1<-get_dist(x)
 y2<-as.matrix(y1)
 d1<-data.frame(data=seq(1,nrow(p2),1),clusters=rep(0,nrow(p2)))
 for(i in 1:nrow(p2))
   
 {
   d1[i,2]<-which.min(y2[i+2,1:2])
 }
y3<-as.data.frame(cbind(Cereals_scaled,clust))  
cbind(y3$clust[56:74],d1$clusters)
table(y3$clust[56:74]==d1$clusters)

## From the above observation, we can see that the clusters are stable
```
 
 
## From the above observation, we can see that the clusters are stable
                                                 
                        
#From the above values, we can say that the clusters are fairly stable.
# 4)Healthy cereals
```{r}
r<-cbind(Nd3,clust)
r[r$clust==1,]
r[r$clust==2,]
```
# Calculating means rating to determine the best cluster.
```{r}
mean(r[r$clust==1,"rating"])
mean(r[r$clust==2,"rating"])
```
# From the above cluster analysis, as Cluster 1 has high value, we can infer this cluster and best fit.


```






